{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install dspy-ai cloudpickle matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\atan\\Documents\\wiqlabs-summarize-verbatim\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import relevant packages\n",
    "import dspy # pip install dspy-ai\n",
    "from dspy.signatures.signature import signature_to_template\n",
    "# Setting up LLM models\n",
    "PROJECT_ID = \"wiq-gen-ai-rd-dev\"\n",
    "MODEL_ID = \"gemini-1.5-flash-001\"\n",
    "\n",
    "flash = dspy.GoogleVertexAI(\n",
    "    model=MODEL_ID, \n",
    "    max_tokens=1000,\n",
    "    temperature=0.05, \n",
    "    project=PROJECT_ID,\n",
    ")\n",
    "\n",
    "# Note: lm is language model and rm is retrieval model\n",
    "dspy.settings.configure(lm=flash, rm=flash)\n",
    "\n",
    "# Fixed themes for the prompt\n",
    "themes = \"\"\"\n",
    "    Technical Learning Resources,\n",
    "    Clarity of Requirements,\n",
    "    Time Commitment / Workload,\n",
    "    Showcase / Presentation Format,\n",
    "    Communication and Information Sharing,\n",
    "    Team Formation and Dynamics,\n",
    "    Relevance of Training Content,\n",
    "    Application to Business / Use Cases,\n",
    "    Accessibility and Inclusion,\n",
    "    Incentives and Recognition,\n",
    "    Post-Hackathon Follow-up,\n",
    "    Support from Leadership / Mentors,\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Training and Testing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"./golden_dataset.csv\")\n",
    "dataset = data[[\"comments\", \"answer\"]]\n",
    "dataset = dataset[\n",
    "    dataset.comments.notna() & dataset.answer.notna()\n",
    "]\n",
    "# dataset = dataset.head(5)\n",
    "\n",
    "# Create training and testing dataset\n",
    "from dspy.datasets import DataLoader\n",
    "\n",
    "dl = DataLoader()\n",
    "hackathon_dataset = dl.from_pandas(dataset, fields=(\"comments\", \"answer\"), input_keys=['comments'])\n",
    "splits = dl.train_test_split(hackathon_dataset, train_size=0.8) # `dataset` is a List of dspy.Example\n",
    "train_dataset = splits['train']\n",
    "test_dataset = splits['test']\n",
    "# train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up signature for DSPy\n",
    "class GenerateTheme(dspy.Signature):\n",
    "    \"\"\"Classify user feedback from hackathon to one single theme\"\"\"\n",
    "    comments = dspy.InputField(desc=\"user feedback from hackathon\")\n",
    "    themes = dspy.OutputField(desc=f\"only pick one of the following choices: {themes}\")\n",
    "\n",
    "\n",
    "class FollowupQuery(dspy.Signature):\n",
    "    \"\"\"Generate a query which is conducive to classifying the comment\"\"\"\n",
    "\n",
    "    context = dspy.InputField(desc=f\"contains relevant facts to classify comments from hackathon to {themes}\")\n",
    "    comments = dspy.InputField(desc=\"user feedback from hackathon\")\n",
    "    search_query = dspy.OutputField(desc=\"Judge if the context is adequate to classify user comments, if not adequate or if it is blank, generate a search query that would help you classify the comments\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vanilla\n",
    "class Vanilla(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.generate_answer = dspy.Predict(GenerateTheme)\n",
    "    \n",
    "    def forward(self, comments):\n",
    "        answer = self.generate_answer(comments=comments)\n",
    "        theme = answer.themes.split(\"Themes: \",1)[1].strip()\n",
    "        return dspy.Prediction(answer=theme)\n",
    "    \n",
    "vanilla = Vanilla()\n",
    "\n",
    "## COT\n",
    "class COT(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.generate_answer = dspy.ChainOfThought(GenerateTheme)\n",
    "    \n",
    "    def forward(self, comments):\n",
    "        answer = self.generate_answer(comments=comments)\n",
    "        return dspy.Prediction(answer=answer.themes)\n",
    "    \n",
    "cot = COT()\n",
    "\n",
    "## ReAct\n",
    "class ReAct(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.generate_answer = dspy.ReAct(GenerateTheme)\n",
    "    \n",
    "    def forward(self, comments):\n",
    "        answer = self.generate_answer(comments=comments)\n",
    "        return dspy.Prediction(answer=answer.themes)\n",
    "    \n",
    "react = ReAct()\n",
    "\n",
    "\n",
    "## BasicMultiHop\n",
    "class BasicMultiHop(dspy.Module):\n",
    "    def __init__(self, num_passages=3):\n",
    "        self.retrieve = dspy.Retrieve(k=num_passages)\n",
    "        self.generate_query = dspy.ChainOfThought(FollowupQuery)\n",
    "        self.generate_answer = dspy.ChainOfThought(GenerateTheme)\n",
    "\n",
    "    def forward(self, comments):\n",
    "        context = []\n",
    "\n",
    "        for hop in range(2):\n",
    "            query = self.generate_query(context=context, comments=comments).search_query\n",
    "            context += self.retrieve(query).passages\n",
    "\n",
    "        answer = self.generate_answer(context=context, comments=comments)\n",
    "        return dspy.Prediction(answer=answer.themes)\n",
    "    \n",
    "multihop = BasicMultiHop(num_passages=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    answer='**Clarity of Requirements, Support from Leadership / Mentors**'\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test a question\n",
    "rag = cot\n",
    "pred = rag.forward(\n",
    "    \"The topic and content was very interesting and the opportunity to connect with technical and commercial mentors was great! In the future, it would be great to see either more structure to the hackathon, or more communication about the structure of the hackathon.\"\n",
    ")\n",
    "pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flash.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then define permutations for our model candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt import LabeledFewShot, BootstrapFewShot\n",
    "\n",
    "metric = dspy.evaluate.metrics.answer_exact_match\n",
    "\n",
    "modules = {\n",
    "    'vanilla': vanilla,\n",
    "    'cot': cot,\n",
    "    'react': react,\n",
    "    'multihop': multihop,\n",
    "}\n",
    "\n",
    "optimizers = {\n",
    "    'none': None,\n",
    "    'labeled_few_shot': LabeledFewShot(),\n",
    "    'bootstrap_few_shot': BootstrapFewShot(metric=metric, max_errors=20),\n",
    "    # 'bootstrap_few_shot_random_search': BootstrapFewShotWithRandomSearch(\n",
    "    #                                         max_bootstrapped_demos=8,\n",
    "    #                                         max_labeled_demos=8,\n",
    "    #                                         num_candidate_programs=10,\n",
    "    #                                         num_threads=8,\n",
    "    #                                         metric=metric,\n",
    "    #                                         teacher_settings=dict(lm=flash)\n",
    "    #                                     )    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a helper class to facilitate the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.evaluate.evaluate import Evaluate\n",
    "import pandas as pd\n",
    "\n",
    "NUM_THREADS = 4\n",
    "class ModelSelection():\n",
    "\n",
    "    # Compile our models\n",
    "    def __init__(self, modules, optimizers, metric, trainset):\n",
    "        self.models = []\n",
    "        self.metric = metric\n",
    "        \n",
    "        for module_name, module in modules.items():\n",
    "            print(f'Compiling models for {module_name}...')\n",
    "            models_for_a_program = {'module_name': module_name, 'optimizers': []}\n",
    "\n",
    "            for optimizer_name, optimizer in optimizers.items():\n",
    "                print(f'...{optimizer_name}')\n",
    "                if optimizer is None:\n",
    "                    compiled_model = module\n",
    "                else:\n",
    "                    compiled_model = optimizer.compile(student=module, trainset=trainset)\n",
    "\n",
    "                optimizer = {\n",
    "                        'name': optimizer_name,\n",
    "                        'compiled_model': compiled_model\n",
    "                }\n",
    "\n",
    "                models_for_a_program['optimizers'].append(optimizer)\n",
    "\n",
    "            self.models.append(models_for_a_program)\n",
    "\n",
    "    # Evaluate our models against the testset. After evaluation, we will have a matrix of models and their scores under the evaluation_matrix attribute\n",
    "    def evaluate(self, testset):\n",
    "        evaluator = Evaluate(devset=testset, metric=self.metric, num_threads=3, return_outputs=True)\n",
    "        for module in self.models:\n",
    "            print(f\"\"\"Evaluating models for {module['module_name']}...\"\"\")\n",
    "            for optimizer in module['optimizers']:\n",
    "                compiled_model = optimizer['compiled_model']\n",
    "                evaluation_score, outputs = evaluator(compiled_model)\n",
    "                optimizer['score'] = evaluation_score\n",
    "\n",
    "        # read dict into a dataframe\n",
    "        df = pd.DataFrame(self.models)\n",
    "\n",
    "        # unnest optimizers column\n",
    "        df = df.explode('optimizers')\n",
    "\n",
    "        # extract name/score column from optimizers\n",
    "        df['optimizer'] = df['optimizers'].apply(lambda x: x['name'])\n",
    "        df['score'] = df['optimizers'].apply(lambda x: x['score'])\n",
    "\n",
    "        df.drop(columns=['optimizers'], inplace=True)\n",
    "        self.evaluation_matrix = df\n",
    "\n",
    "    # Raise a question against the compiled model\n",
    "    def question_for_model(self, module_name, optimizer_name, comments):\n",
    "        for model in self.models:\n",
    "            if model['module_name'] == module_name:\n",
    "                for s in model['optimizers']:\n",
    "                    if s['name'] == optimizer_name:\n",
    "                        return s['compiled_model'](comments=comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the models\n",
    "ms = ModelSelection(modules=modules, optimizers=optimizers, metric=metric, trainset=train_dataset)\n",
    "\n",
    "# Evaluate them\n",
    "ms.evaluate(testset=test_dataset)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>module_name</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cot</td>\n",
       "      <td>mipro</td>\n",
       "      <td>66.67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  module_name optimizer  score\n",
       "0         cot     mipro  66.67"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms.evaluation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    answer='**Showcase / Presentation Format**'\n",
       ")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = ms.question_for_model(\"cot\", \"mipro\",\"Slightly fewer final presentations, and each presenting team presenting for slightly longer\")\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "## PROPOSED INSTRUCTION:\n",
      "\n",
      "**You are provided a participant's feedback from a Generative AI Hackathon. Analyze the feedback and choose one of the following themes that best describes its core concern.** \n",
      "\n",
      "**Themes:**\n",
      "\n",
      "- Technical Learning Resources\n",
      "- Clarity of Requirements\n",
      "- Time Commitment / Workload\n",
      "- Showcase / Presentation Format\n",
      "- Communication and Information Sharing\n",
      "- Team Formation and Dynamics\n",
      "- Relevance of Training Content\n",
      "- Application to Business / Use Cases\n",
      "- Accessibility and Inclusion\n",
      "- Incentives and Recognition\n",
      "- Post-Hackathon Follow-up\n",
      "- Support from Leadership / Mentors\n",
      "\n",
      "**Explain your choice of theme and provide a detailed reasoning as to why the chosen theme best reflects the feedback.**\n",
      "\n",
      "**Example of participant feedback:** \"I really appreciated the opportunity to learn about the latest generative AI models, however, it would have been more helpful to have hands-on projects using these models.  I felt like the workshop was too theoretical and didn't provide enough real-world application.\"\n",
      "\n",
      "**Your Output:** **Themes: Relevance of Training Content.**\n",
      "\n",
      "**Reasoning: The feedback expresses a concern about the lack of balance between theoretical learning and practical implementation. The participant valued learning new skills but felt the content needed to be more relevant to real-world situations. Therefore, the core concern expressed by the feedback falls under 'Relevance of Training Content' theme.**\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Comments: user feedback from hackathon\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the themes}. We ...\n",
      "\n",
      "Themes: only pick one of the following choices: Technical Learning Resources, Clarity of Requirements, Time Commitment / Workload, Showcase / Presentation Format, Communication and Information Sharing, Team Formation and Dynamics, Relevance of Training Content, Application to Business / Use Cases, Accessibility and Inclusion, Incentives and Recognition, Post-Hackathon Follow-up, Support from Leadership / Mentors,\n",
      "\n",
      "---\n",
      "\n",
      "Comments: Slightly fewer final presentations, and each presenting team presenting for slightly longer\n",
      "\n",
      "Reasoning: Let's think step by step in order to Comments: Slightly fewer final presentations, and each presenting team presenting for slightly longer Reasoning: Let's think step by step in order to **identify the theme**. The feedback focuses on the **structure and duration of the presentation portion of the hackathon**. This suggests the theme is **Showcase / Presentation Format**.\n",
      "\n",
      "Themes: **Showcase / Presentation Format**\n",
      "\n",
      "---\n",
      "\n",
      "Comments: Feel like more info could have been given out in leading up to the pitch submission date.\n",
      "\n",
      "Reasoning: Let's think step by step in order to Comments: Feel like more info could have been given out in leading up to the pitch submission date. Reasoning: Let's think step by step in order to **identify the theme**. The comment focuses on the lack of information provided before the pitch submission deadline. This suggests an issue with the **communication and information sharing** during the hackathon.\n",
      "\n",
      "Themes: **Communication and Information Sharing**\n",
      "\n",
      "---\n",
      "\n",
      "Comments: was awesome! I'd like to keep hearing about gen ai initiatives within Quantium though. It would be a shame if there was radio silence on the topic once the hackathon was done :). Thanks so much\n",
      "\n",
      "Reasoning: Let's think step by step in order to Comments: was awesome! I'd like to keep hearing about gen ai initiatives within Quantium though. It would be a shame if there was radio silence on the topic once the hackathon was done :). Thanks so much Reasoning: Let's think step by step in order to **produce the themes**. We are looking for a single theme that best encapsulates the feedback. The comment expresses a desire for continued engagement and communication about the topic of generative AI, even after the hackathon. This suggests a need for **Post-Hackathon Follow-up**.\n",
      "\n",
      "Themes: **Post-Hackathon Follow-up**\n",
      "\n",
      "---\n",
      "\n",
      "Comments: Slightly fewer final presentations, and each presenting team presenting for slightly longer\n",
      "\n",
      "Reasoning: Let's think step by step in order toComments: Slightly fewer final presentations, and each presenting team presenting for slightly longer\n",
      "\n",
      "Reasoning: Let's think step by step in order to **identify the theme**. The feedback focuses on the **structure and duration of the presentation portion of the hackathon**. This suggests the theme is **Showcase / Presentation Format**.\n",
      "\n",
      "Themes: **Showcase / Presentation Format** \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n## PROPOSED INSTRUCTION:\\n\\n**You are provided a participant\\'s feedback from a Generative AI Hackathon. Analyze the feedback and choose one of the following themes that best describes its core concern.** \\n\\n**Themes:**\\n\\n- Technical Learning Resources\\n- Clarity of Requirements\\n- Time Commitment / Workload\\n- Showcase / Presentation Format\\n- Communication and Information Sharing\\n- Team Formation and Dynamics\\n- Relevance of Training Content\\n- Application to Business / Use Cases\\n- Accessibility and Inclusion\\n- Incentives and Recognition\\n- Post-Hackathon Follow-up\\n- Support from Leadership / Mentors\\n\\n**Explain your choice of theme and provide a detailed reasoning as to why the chosen theme best reflects the feedback.**\\n\\n**Example of participant feedback:** \"I really appreciated the opportunity to learn about the latest generative AI models, however, it would have been more helpful to have hands-on projects using these models.  I felt like the workshop was too theoretical and didn\\'t provide enough real-world application.\"\\n\\n**Your Output:** **Themes: Relevance of Training Content.**\\n\\n**Reasoning: The feedback expresses a concern about the lack of balance between theoretical learning and practical implementation. The participant valued learning new skills but felt the content needed to be more relevant to real-world situations. Therefore, the core concern expressed by the feedback falls under \\'Relevance of Training Content\\' theme.**\\n\\n---\\n\\nFollow the following format.\\n\\nComments: user feedback from hackathon\\n\\nReasoning: Let\\'s think step by step in order to ${produce the themes}. We ...\\n\\nThemes: only pick one of the following choices: Technical Learning Resources, Clarity of Requirements, Time Commitment / Workload, Showcase / Presentation Format, Communication and Information Sharing, Team Formation and Dynamics, Relevance of Training Content, Application to Business / Use Cases, Accessibility and Inclusion, Incentives and Recognition, Post-Hackathon Follow-up, Support from Leadership / Mentors,\\n\\n---\\n\\nComments: Slightly fewer final presentations, and each presenting team presenting for slightly longer\\n\\nReasoning: Let\\'s think step by step in order to Comments: Slightly fewer final presentations, and each presenting team presenting for slightly longer Reasoning: Let\\'s think step by step in order to **identify the theme**. The feedback focuses on the **structure and duration of the presentation portion of the hackathon**. This suggests the theme is **Showcase / Presentation Format**.\\n\\nThemes: **Showcase / Presentation Format**\\n\\n---\\n\\nComments: Feel like more info could have been given out in leading up to the pitch submission date.\\n\\nReasoning: Let\\'s think step by step in order to Comments: Feel like more info could have been given out in leading up to the pitch submission date. Reasoning: Let\\'s think step by step in order to **identify the theme**. The comment focuses on the lack of information provided before the pitch submission deadline. This suggests an issue with the **communication and information sharing** during the hackathon.\\n\\nThemes: **Communication and Information Sharing**\\n\\n---\\n\\nComments: was awesome! I\\'d like to keep hearing about gen ai initiatives within Quantium though. It would be a shame if there was radio silence on the topic once the hackathon was done :). Thanks so much\\n\\nReasoning: Let\\'s think step by step in order to Comments: was awesome! I\\'d like to keep hearing about gen ai initiatives within Quantium though. It would be a shame if there was radio silence on the topic once the hackathon was done :). Thanks so much Reasoning: Let\\'s think step by step in order to **produce the themes**. We are looking for a single theme that best encapsulates the feedback. The comment expresses a desire for continued engagement and communication about the topic of generative AI, even after the hackathon. This suggests a need for **Post-Hackathon Follow-up**.\\n\\nThemes: **Post-Hackathon Follow-up**\\n\\n---\\n\\nComments: Slightly fewer final presentations, and each presenting team presenting for slightly longer\\n\\nReasoning: Let\\'s think step by step in order to\\x1b[32mComments: Slightly fewer final presentations, and each presenting team presenting for slightly longer\\n\\nReasoning: Let\\'s think step by step in order to **identify the theme**. The feedback focuses on the **structure and duration of the presentation portion of the hackathon**. This suggests the theme is **Showcase / Presentation Format**.\\n\\nThemes: **Showcase / Presentation Format** \\n\\x1b[0m\\n\\n\\n'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flash.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Phoenix, DSPy, and other dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \"regex~=2023.10.3\" dspy-ai\n",
    "# !pip install arize-phoenix openinference-instrumentation-vertexai vertexai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌍 To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "📖 For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
     ]
    }
   ],
   "source": [
    "# Phoenix by default uses the 6006 port for the UI. \n",
    "# If you have a port conflict, you can close the port by uncommenting the following code\n",
    "\n",
    "import phoenix as px\n",
    "phoenix_session = px.launch_app()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\atan\\Documents\\wiqlabs-summarize-verbatim\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenTelemetry Tracing Details\n",
      "|  Phoenix Project: default\n",
      "|  Span Processor: SimpleSpanProcessor\n",
      "|  Collector Endpoint: localhost:4317\n",
      "|  Transport: gRPC\n",
      "|  Transport Headers: {'user-agent': '****'}\n",
      "|  \n",
      "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
      "|  \n",
      "|  `register` has set this TracerProvider as the global OpenTelemetry default.\n",
      "|  To disable this behavior, call `register` with `set_global_tracer_provider=False`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from phoenix.otel import register\n",
    "from openinference.instrumentation.vertexai import VertexAIInstrumentor\n",
    "tracer_provider = register(\n",
    "  project_name=\"default\", # Default is 'default'\n",
    ")  \n",
    "VertexAIInstrumentor().instrument(tracer_provider=tracer_provider)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing port 6006 by terminating PID 18324\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Used to kill phoenix\n",
    "import psutil\n",
    "\n",
    "def close_port(port):\n",
    "    for conn in psutil.net_connections(kind='inet'):\n",
    "        if conn.laddr.port == port:\n",
    "            print(f\"Closing port {port} by terminating PID {conn.pid}\")\n",
    "            process = psutil.Process(conn.pid)\n",
    "            process.terminate()\n",
    "\n",
    "close_port(6006)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
