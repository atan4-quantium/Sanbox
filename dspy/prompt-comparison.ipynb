{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install dspy-ai cloudpickle matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant packages\n",
    "import dspy\n",
    "from dspy.signatures.signature import signature_to_template\n",
    "# Setting up LLM models\n",
    "PROJECT_ID = os.environ.get(\"PROJECT_ID\")\n",
    "MODEL_ID = \"gemini-1.5-flash-001\"\n",
    "\n",
    "flash = dspy.GoogleVertexAI(\n",
    "    model=MODEL_ID, \n",
    "    max_tokens=1000,\n",
    "    temperature=0.05, \n",
    "    project=PROJECT_ID,\n",
    ")\n",
    "\n",
    "# Note: different models can be set for prompt model and task model\n",
    "# - prompt model : rm (retrieval model) \n",
    "# - task model : lm (language model)\n",
    "dspy.settings.configure(lm=flash, rm=flash)\n",
    "\n",
    "# Fixed themes for the prompt\n",
    "themes = \"\"\"\n",
    "    Technical Learning Resources,\n",
    "    Clarity of Requirements,\n",
    "    Time Commitment / Workload,\n",
    "    Showcase / Presentation Format,\n",
    "    Communication and Information Sharing,\n",
    "    Team Formation and Dynamics,\n",
    "    Relevance of Training Content,\n",
    "    Application to Business / Use Cases,\n",
    "    Accessibility and Inclusion,\n",
    "    Incentives and Recognition,\n",
    "    Post-Hackathon Follow-up,\n",
    "    Support from Leadership / Mentors,\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Define Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up signature for DSPy\n",
    "class GenerateTheme(dspy.Signature):\n",
    "    \"\"\"Classify user feedback from hackathon to one single theme\"\"\"\n",
    "    comments = dspy.InputField(desc=\"user feedback from hackathon\")\n",
    "    themes = dspy.OutputField(desc=f\"only pick one of the following choices: {themes}\")\n",
    "\n",
    "\n",
    "class FollowupQuery(dspy.Signature):\n",
    "    \"\"\"Generate a query which is conducive to classifying the comment\"\"\"\n",
    "\n",
    "    context = dspy.InputField(desc=f\"contains relevant facts to classify comments from hackathon to {themes}\")\n",
    "    comments = dspy.InputField(desc=\"user feedback from hackathon\")\n",
    "    search_query = dspy.OutputField(desc=\"Judge if the context is adequate to classify user comments, if not adequate or if it is blank, generate a search query that would help you classify the comments\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vanilla\n",
    "class Vanilla(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.generate_answer = dspy.Predict(GenerateTheme)\n",
    "    \n",
    "    def forward(self, comments):\n",
    "        answer = self.generate_answer(comments=comments)\n",
    "        theme = answer.themes.split(\"Themes: \",1)[1].strip()\n",
    "        return dspy.Prediction(answer=theme)\n",
    "    \n",
    "vanilla = Vanilla()\n",
    "\n",
    "## COT\n",
    "class COT(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.generate_answer = dspy.ChainOfThought(GenerateTheme)\n",
    "    \n",
    "    def forward(self, comments):\n",
    "        answer = self.generate_answer(comments=comments)\n",
    "        return dspy.Prediction(answer=answer.themes)\n",
    "    \n",
    "cot = COT()\n",
    "\n",
    "## ReAct\n",
    "class ReAct(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.generate_answer = dspy.ReAct(GenerateTheme)\n",
    "    \n",
    "    def forward(self, comments):\n",
    "        answer = self.generate_answer(comments=comments)\n",
    "        return dspy.Prediction(answer=answer.themes)\n",
    "    \n",
    "react = ReAct()\n",
    "\n",
    "\n",
    "## BasicMultiHop\n",
    "class BasicMultiHop(dspy.Module):\n",
    "    def __init__(self, num_passages=3):\n",
    "        self.retrieve = dspy.Retrieve(k=num_passages)\n",
    "        self.generate_query = dspy.ChainOfThought(FollowupQuery)\n",
    "        self.generate_answer = dspy.ChainOfThought(GenerateTheme)\n",
    "\n",
    "    def forward(self, comments):\n",
    "        context = []\n",
    "\n",
    "        for hop in range(2):\n",
    "            query = self.generate_query(context=context, comments=comments).search_query\n",
    "            context += self.retrieve(query).passages\n",
    "\n",
    "        answer = self.generate_answer(context=context, comments=comments)\n",
    "        return dspy.Prediction(answer=answer.themes)\n",
    "    \n",
    "multihop = BasicMultiHop(num_passages=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a question\n",
    "rag = cot\n",
    "pred = rag.forward(\n",
    "    \"The topic and content was very interesting and the opportunity to connect with technical and commercial mentors was great! In the future, it would be great to see either more structure to the hackathon, or more communication about the structure of the hackathon.\"\n",
    ")\n",
    "pred\n",
    "# Check prompt history\n",
    "# flash.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Import training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and testing dataset\n",
    "import pandas as pd\n",
    "from dspy.datasets import DataLoader\n",
    "\n",
    "# Convert csv to dataframe\n",
    "data = pd.read_csv(\"./data.csv\")\n",
    "dataset = data[[\"comments\", \"answer\"]]\n",
    "dataset = dataset[\n",
    "    dataset.comments.notna() & dataset.answer.notna()\n",
    "]\n",
    "# dataset = dataset.head(5) # limit dataset for testing\n",
    "\n",
    "# Load data into the desired format for DSPy\n",
    "dl = DataLoader()\n",
    "dspy_dataset = dl.from_pandas(dataset, fields=(\"comments\", \"answer\"), input_keys=['comments'])\n",
    "splits = dl.train_test_split(dspy_dataset, train_size=0.8) # `dataset` is a List of dspy.Example\n",
    "train_dataset = splits['train']\n",
    "test_dataset = splits['test']\n",
    "\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Define permutations for our model candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt import LabeledFewShot, BootstrapFewShot\n",
    "\n",
    "metric = dspy.evaluate.metrics.answer_exact_match\n",
    "\n",
    "modules = {\n",
    "    'vanilla': vanilla,\n",
    "    'cot': cot,\n",
    "    'react': react,\n",
    "    'multihop': multihop,\n",
    "}\n",
    "\n",
    "optimizers = {\n",
    "    'none': None,\n",
    "    'labeled_few_shot': LabeledFewShot(),\n",
    "    'bootstrap_few_shot': BootstrapFewShot(metric=metric, max_errors=20),\n",
    "    # 'bootstrap_few_shot_random_search': BootstrapFewShotWithRandomSearch(\n",
    "    #                                         max_bootstrapped_demos=8,\n",
    "    #                                         max_labeled_demos=8,\n",
    "    #                                         num_candidate_programs=10,\n",
    "    #                                         num_threads=8,\n",
    "    #                                         metric=metric,\n",
    "    #                                         teacher_settings=dict(lm=flash)\n",
    "    #                                     )    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a helper class to facilitate the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.evaluate.evaluate import Evaluate\n",
    "import pandas as pd\n",
    "\n",
    "NUM_THREADS = 4\n",
    "class ModelSelection():\n",
    "\n",
    "    # Compile our models\n",
    "    def __init__(self, modules, optimizers, metric, trainset):\n",
    "        self.models = []\n",
    "        self.metric = metric\n",
    "        \n",
    "        for module_name, module in modules.items():\n",
    "            print(f'Compiling models for {module_name}...')\n",
    "            models_for_a_program = {'module_name': module_name, 'optimizers': []}\n",
    "\n",
    "            for optimizer_name, optimizer in optimizers.items():\n",
    "                print(f'...{optimizer_name}')\n",
    "                if optimizer is None:\n",
    "                    compiled_model = module\n",
    "                else:\n",
    "                    compiled_model = optimizer.compile(student=module, trainset=trainset)\n",
    "\n",
    "                optimizer = {\n",
    "                        'name': optimizer_name,\n",
    "                        'compiled_model': compiled_model\n",
    "                }\n",
    "\n",
    "                models_for_a_program['optimizers'].append(optimizer)\n",
    "\n",
    "            self.models.append(models_for_a_program)\n",
    "\n",
    "    # Evaluate our models against the testset. After evaluation, we will have a matrix of models and their scores under the evaluation_matrix attribute\n",
    "    def evaluate(self, testset):\n",
    "        evaluator = Evaluate(devset=testset, metric=self.metric, num_threads=3, return_outputs=True)\n",
    "        for module in self.models:\n",
    "            print(f\"\"\"Evaluating models for {module['module_name']}...\"\"\")\n",
    "            for optimizer in module['optimizers']:\n",
    "                compiled_model = optimizer['compiled_model']\n",
    "                evaluation_score, outputs = evaluator(compiled_model)\n",
    "                optimizer['score'] = evaluation_score\n",
    "\n",
    "        # read dict into a dataframe\n",
    "        df = pd.DataFrame(self.models)\n",
    "\n",
    "        # unnest optimizers column\n",
    "        df = df.explode('optimizers')\n",
    "\n",
    "        # extract name/score column from optimizers\n",
    "        df['optimizer'] = df['optimizers'].apply(lambda x: x['name'])\n",
    "        df['score'] = df['optimizers'].apply(lambda x: x['score'])\n",
    "\n",
    "        df.drop(columns=['optimizers'], inplace=True)\n",
    "        self.evaluation_matrix = df\n",
    "\n",
    "    # Raise a question against the compiled model\n",
    "    def question_for_model(self, module_name, optimizer_name, comments):\n",
    "        for model in self.models:\n",
    "            if model['module_name'] == module_name:\n",
    "                for s in model['optimizers']:\n",
    "                    if s['name'] == optimizer_name:\n",
    "                        return s['compiled_model'](comments=comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Train and evaluate the different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the models\n",
    "ms = ModelSelection(modules=modules, optimizers=optimizers, metric=metric, trainset=train_dataset)\n",
    "\n",
    "# Evaluate them\n",
    "ms.evaluate(testset=test_dataset)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms.evaluation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with compiled model\n",
    "pred = ms.question_for_model(\"cot\", \"labeled_few_shot\", \"Slightly fewer final presentations, and each presenting team presenting for slightly longer\")\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flash.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional) Install Phoenix to inspect history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \"regex~=2023.10.3\" arize-phoenix openinference-instrumentation-vertexai vertexai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phoenix by default uses the 6006 port for the UI. \n",
    "# If you have a port conflict, you can close the port by uncommenting the following code\n",
    "\n",
    "import phoenix as px\n",
    "phoenix_session = px.launch_app()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.otel import register\n",
    "from openinference.instrumentation.vertexai import VertexAIInstrumentor\n",
    "tracer_provider = register(\n",
    "  project_name=\"default\", # Default is 'default'\n",
    ")  \n",
    "VertexAIInstrumentor().instrument(tracer_provider=tracer_provider)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to kill phoenix\n",
    "import psutil\n",
    "\n",
    "def close_port(port):\n",
    "    for conn in psutil.net_connections(kind='inet'):\n",
    "        if conn.laddr.port == port:\n",
    "            print(f\"Closing port {port} by terminating PID {conn.pid}\")\n",
    "            process = psutil.Process(conn.pid)\n",
    "            process.terminate()\n",
    "\n",
    "close_port(6006)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
