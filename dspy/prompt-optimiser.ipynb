{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install dspy-ai cloudpickle matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant packages\n",
    "import dspy\n",
    "from dspy.signatures.signature import signature_to_template\n",
    "\n",
    "# Setting up LLM models\n",
    "PROJECT_ID = os.environ.get(\"PROJECT_ID\")\n",
    "MODEL_ID = \"gemini-1.5-flash-001\"\n",
    "\n",
    "flash = dspy.GoogleVertexAI(\n",
    "    model=MODEL_ID, \n",
    "    max_tokens=1000,\n",
    "    temperature=0.05, \n",
    "    project=PROJECT_ID\n",
    ")\n",
    "\n",
    "# Note: different models can be set for prompt model and task model\n",
    "# - prompt model : rm (retrieval model) \n",
    "# - task model : lm (language model)\n",
    "dspy.settings.configure(lm=flash, rm=flash)\n",
    "\n",
    "# Fixed themes used for signatures\n",
    "themes = \"\"\"\n",
    "    Technical Learning Resources,\n",
    "    Clarity of Requirements,\n",
    "    Time Commitment / Workload,\n",
    "    Showcase / Presentation Format,\n",
    "    Communication and Information Sharing,\n",
    "    Team Formation and Dynamics,\n",
    "    Relevance of Training Content,\n",
    "    Application to Business / Use Cases,\n",
    "    Accessibility and Inclusion,\n",
    "    Incentives and Recognition,\n",
    "    Post-Hackathon Follow-up,\n",
    "    Support from Leadership / Mentors,\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Define Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up signature for DSPy\n",
    "class GenerateTheme(dspy.Signature):\n",
    "    \"\"\"Classify user feedback from hackathon to one single theme\"\"\"\n",
    "    comments = dspy.InputField(desc=\"user feedback from hackathon\")\n",
    "    themes = dspy.OutputField(desc=f\"only pick one of the following choices: {themes}\")\n",
    "\n",
    "## COT\n",
    "class COT(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.generate_answer = dspy.ChainOfThought(GenerateTheme)\n",
    "    \n",
    "    def forward(self, comments):\n",
    "        answer = self.generate_answer(comments=comments)\n",
    "        return dspy.Prediction(answer=answer.themes)\n",
    "    \n",
    "cot = COT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a question\n",
    "pred = cot.forward(\n",
    "    \"The topic and content was very interesting and the opportunity to connect with technical and commercial mentors was great! In the future, it would be great to see either more structure to the hackathon, or more communication about the structure of the hackathon.\"\n",
    ")\n",
    "pred\n",
    "# Check prompt history\n",
    "# flash.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Import training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and testing dataset\n",
    "import pandas as pd\n",
    "from dspy.datasets import DataLoader\n",
    "\n",
    "# Convert csv to dataframe\n",
    "data = pd.read_csv(\"./data.csv\")\n",
    "dataset = data[[\"comments\", \"answer\"]] # only want the relevant columns\n",
    "dataset = dataset[\n",
    "    dataset.comments.notna() & dataset.answer.notna()\n",
    "]\n",
    "# dataset = dataset.head(5) # limit dataset for testing\n",
    "\n",
    "# Load data into the desired format for DSPy\n",
    "dl = DataLoader()\n",
    "dspy_dataset = dl.from_pandas(dataset, fields=(\"comments\", \"answer\"), input_keys=['comments']) # IMPORTANT: input_keys is the 'input' for the model\n",
    "splits = dl.train_test_split(dspy_dataset, train_size=0.8) # `dataset` is a List of dspy.Example\n",
    "train_dataset = splits['train']\n",
    "test_dataset = splits['test']\n",
    "\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Evaluate baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the accuracy - calculates the HIT ratio (i.e. exact match of the predicted answer)\n",
    "from dspy.evaluate.evaluate import Evaluate\n",
    "\n",
    "# Set up the `evaluate_rag` function. We'll use this many times below.\n",
    "evaluate_rag = Evaluate(devset=test_dataset, num_threads=1, display_progress=True, display_table=5)\n",
    "\n",
    "# Evaluate the `compiled_rag` program with the `answer_exact_match` metric.\n",
    "metric = dspy.evaluate.answer_exact_match\n",
    "evaluation_score = evaluate_rag(cot, metric=metric)\n",
    "print(f\"Retrieval HIT Ratio for RAG: {evaluation_score}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Compiling with Optimiser MIPRO (automates prompts and examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt import MIPROv2\n",
    "\n",
    "# Constants for optimisers\n",
    "num_threads = 2\n",
    "kwargs = dict(num_threads=num_threads, display_progress=True, display_table=5)\n",
    "metric = dspy.evaluate.answer_exact_match\n",
    "\n",
    "optimiser = MIPROv2(\n",
    "                task_model=flash,\n",
    "                prompt_model=flash,\n",
    "                metric=metric,\n",
    "                log_dir=\"logs/\"\n",
    "            )\n",
    "\n",
    "compiled_model = optimiser.compile(\n",
    "                    student=cot, \n",
    "                    trainset=train_dataset, \n",
    "                    max_bootstrapped_demos=4, \n",
    "                    max_labeled_demos=4, \n",
    "                    eval_kwargs=kwargs,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the new compiled model\n",
    "pred = compiled_model.forward(\n",
    "    \"The topic and content was very interesting and the opportunity to connect with technical and commercial mentors was great! In the future, it would be great to see either more structure to the hackathon, or more communication about the structure of the hackathon.\"\n",
    ")\n",
    "pred\n",
    "flash.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Plot model logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_model_logs(trial_logs):\n",
    "    # Extracting trial numbers, scores, and pruning status\n",
    "    trial_numbers = list(trial_logs.keys())\n",
    "    scores = [trial_logs[trial]['score'] for trial in trial_numbers]\n",
    "    pruning_status = [trial_logs[trial]['pruned'] for trial in trial_numbers]\n",
    "\n",
    "    # Plot setup\n",
    "    plt.figure(figsize=(5, 3))\n",
    "\n",
    "    # Plotting each point\n",
    "    for trial_number, score, pruned in zip(trial_numbers, scores, pruning_status):\n",
    "        if pruned:\n",
    "            plt.scatter(trial_number, score, color='grey', label='Pruned Batch' if 'Pruned Batch' not in plt.gca().get_legend_handles_labels()[1] else \"\")\n",
    "        else:\n",
    "            plt.scatter(trial_number, score, color='green', label='Successful Batch' if 'Successful Batch' not in plt.gca().get_legend_handles_labels()[1] else \"\")\n",
    "\n",
    "    plt.xlabel('Batch Number')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Batch Scores')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_model_logs(compiled_model.trial_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_logs(model, compiled_model):\n",
    "    # Init constant\n",
    "    best_score = 0\n",
    "\n",
    "    def get_signature(predictor):\n",
    "        if (hasattr(predictor, 'extended_signature')):\n",
    "            return predictor.extended_signature\n",
    "        elif (hasattr(predictor, 'signature')):\n",
    "            return predictor.signature\n",
    "\n",
    "    print(f\"Baseline program | Score: {best_score}:\")\n",
    "    for i,predictor in enumerate(model.predictors()):\n",
    "        print(f\"Prompt {i+1} Instruction: {get_signature(predictor).instructions}\")\n",
    "    print()\n",
    "\n",
    "    print(\"----------------\")\n",
    "\n",
    "    for trial_num in compiled_model.trial_logs:\n",
    "        program_score = compiled_model.trial_logs[trial_num][\"score\"]\n",
    "        program_pruned = compiled_model.trial_logs[trial_num][\"pruned\"]\n",
    "        if program_score > best_score and not program_pruned and compiled_model.trial_logs[trial_num][\"full_eval\"]:\n",
    "            best_score = program_score\n",
    "            best_program_so_far = compiled_model.trial_logs[trial_num][\"program\"]\n",
    "        if trial_num % 5 == 0:\n",
    "            print(f\"Best program after {trial_num} batches | Score: {best_score}:\")\n",
    "            for i,predictor in enumerate(best_program_so_far.predictors()):\n",
    "                print(f\"Prompt {i+1} Instruction: {get_signature(predictor).instructions}\")\n",
    "            print()\n",
    "\n",
    "print_model_logs(cot, compiled_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Saving and loading compiled models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudpickle as pickle\n",
    "\n",
    "LOAD_PRECOMPILED_MODEL = True\n",
    "\n",
    "# saving trial logs\n",
    "def save_trial_logs(model, outputfile=\"trial_logs\"):        \n",
    "    logs = {\n",
    "        index: {key: value for key, value in item.items() if key != \"program\"}\n",
    "        for index, item in model.trial_logs.items()\n",
    "    }\n",
    "    \n",
    "    with open(f\"{outputfile}.pickle\", \"wb\") as file:\n",
    "        pickle.dump(logs, file)\n",
    "\n",
    "# loading trial logs\n",
    "def load_trial_logs(inputfile=\"trial_logs\"):\n",
    "    with open(f\"{inputfile}.pickle\", \"rb\") as file:\n",
    "        logs = pickle.load(file)\n",
    "    \n",
    "    return logs\n",
    "\n",
    "if LOAD_PRECOMPILED_MODEL:\n",
    "    loaded_model = cot.deepcopy()\n",
    "    loaded_model.load('compiled_model.dspy')\n",
    "    trial_logs = load_trial_logs()\n",
    "    loaded_model.trial_logs = trial_logs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Scoring with compiled models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data.csv\") # change this\n",
    "\n",
    "# Scoring locally\n",
    "data[\"prediction\"] = None\n",
    "for index, row in data.iterrows():\n",
    "    if isinstance(row[\"comments\"], str):        \n",
    "        prediction = loaded_model.forward(row[\"comments\"])\n",
    "        data.at[index, \"prediction\"] = prediction.answer.strip(\"**\")\n",
    "\n",
    "# Saving new dataframe\n",
    "data.to_csv('data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
